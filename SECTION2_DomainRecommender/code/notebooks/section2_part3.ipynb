{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f904dc4",
   "metadata": {},
   "source": [
    "# Section 2 Part 3: Collaborative Filtering and Hybrid Approach \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc544e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### 8. Collaborative Filtering Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf0f40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### 8.1. Implement ONE CF approach: \n",
    "- Item-based CF\n",
    "- Use cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780e2fa",
   "metadata": {},
   "source": [
    "$$\\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5a6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_scratch(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors from scratch.\n",
    "    \n",
    "    Args:\n",
    "        vec_a (array-like): First vector.\n",
    "        vec_b (array-like): Second vector.\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity score (between -1 and 1).\n",
    "    \"\"\"\n",
    "    a = np.array(vec_a)\n",
    "    b = np.array(vec_b)\n",
    "    \n",
    "    dot_product = np.dot(a, b)\n",
    "    \n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510d889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Columns: ['fit', 'user_id', 'bust size', 'item_id', 'weight', 'rating', 'rented for', 'review_text', 'body type', 'review_summary', 'category', 'height', 'size', 'age', 'review_date', 'rating_scaled', 'rating_bin']\n",
      "\n",
      "User-Item Matrix Shape: (105508, 5850)\n",
      "Users: 105508, Items: 5850\n",
      "\n",
      "Computing Item-Item Cosine Similarity (From Scratch)...\n",
      "Item Similarity Matrix (CF) Computed.\n",
      "item_id    123373    123793    124204    124553    125424\n",
      "item_id                                                  \n",
      "123373   1.000000  0.003719  0.007813  0.004896  0.007156\n",
      "123793   0.003719  1.000000  0.002509  0.004930  0.004605\n",
      "124204   0.007813  0.002509  1.000000  0.011039  0.004293\n",
      "124553   0.004896  0.004930  0.011039  1.000000  0.032442\n",
      "125424   0.007156  0.004605  0.004293  0.032442  1.000000\n",
      "\n",
      "--- Recommendations for User 691468 (CF Item-Based) ---\n",
      "1. Item 647288 (Predicted Rating: 10.00)\n",
      "2. Item 2117425 (Predicted Rating: 10.00)\n",
      "3. Item 155820 (Predicted Rating: 10.00)\n",
      "4. Item 163328 (Predicted Rating: 10.00)\n",
      "5. Item 237896 (Predicted Rating: 10.00)\n",
      "\n",
      "CF Item Similarity Matrix saved to 'dataset/item_similarity_cf.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "df = pd.read_csv('dataset/preprocessed_data.csv')\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "if 'rating' not in df.columns:\n",
    "    print(\"Warning: 'rating' column not found. Please ensure column names match Part 1.\")\n",
    "\n",
    "\n",
    "user_item_matrix = df.pivot_table(\n",
    "    index='user_id', \n",
    "    columns='item_id', \n",
    "    values='rating', \n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"\\nUser-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "print(f\"Users: {user_item_matrix.shape[0]}, Items: {user_item_matrix.shape[1]}\")\n",
    "\n",
    "print(\"\\nComputing Item-Item Cosine Similarity (From Scratch)...\")\n",
    "\n",
    "item_matrix_np = user_item_matrix.T.values \n",
    "\n",
    "\n",
    "numerator = np.dot(item_matrix_np, item_matrix_np.T)\n",
    "\n",
    "magnitudes = np.sqrt(np.sum(item_matrix_np**2, axis=1))\n",
    "\n",
    "\n",
    "denominator = np.outer(magnitudes, magnitudes)\n",
    "\n",
    "item_similarity_cf = numerator / (denominator + 1e-9)\n",
    "\n",
    "item_similarity_cf_df = pd.DataFrame(\n",
    "    item_similarity_cf,\n",
    "    index=user_item_matrix.columns,\n",
    "    columns=user_item_matrix.columns\n",
    ")\n",
    "\n",
    "print(\"Item Similarity Matrix (CF) Computed.\")\n",
    "print(item_similarity_cf_df.iloc[:5, :5])\n",
    "\n",
    "def recommend_items_cf(user_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommends items based on the user's past ratings and item similarity.\n",
    "    Formula: Predicted_Rating(u, i) = Weighted Sum of ratings given by u to items similar to i.\n",
    "    \"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        return []\n",
    "\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0].index.tolist()\n",
    "    \n",
    "    item_scores = {}\n",
    "    \n",
    "    for candidate_item in user_item_matrix.columns:\n",
    "        if candidate_item in rated_items:\n",
    "            continue \n",
    "        \n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        \n",
    "        for rated_item in rated_items:\n",
    "            similarity_score = item_similarity_cf_df.loc[candidate_item, rated_item]\n",
    "            \n",
    "            if similarity_score > 0:\n",
    "                user_rating = user_ratings[rated_item]\n",
    "                numerator += similarity_score * user_rating\n",
    "                denominator += similarity_score\n",
    "        \n",
    "        if denominator > 0:\n",
    "            predicted_score = numerator / denominator\n",
    "            item_scores[candidate_item] = predicted_score\n",
    "    \n",
    "    sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_items[:top_n]\n",
    "\n",
    "sample_user = df['user_id'].value_counts().idxmax() \n",
    "print(f\"\\n--- Recommendations for User {sample_user} (CF Item-Based) ---\")\n",
    "\n",
    "recommendations = recommend_items_cf(sample_user, top_n=5)\n",
    "\n",
    "for i, (item, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Item {item} (Predicted Rating: {score:.2f})\")\n",
    "\n",
    "item_similarity_cf_df.to_csv('dataset/item_similarity_cf.csv')\n",
    "print(\"\\nCF Item Similarity Matrix saved to 'dataset/item_similarity_cf.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e271ecc",
   "metadata": {},
   "source": [
    "##### 8.2. Use matrix factorization from section 1:\n",
    "- Apply SVD with k=10 or k=20 latent factors\n",
    "- Generate predictions for target users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56df2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "8.2 Matrix Factorization (SVD) Approach\n",
      "==================================================\n",
      "Matrix prepared for SVD. Shape: (105508, 5850)\n",
      "Applying Truncated SVD with k=20 latent factors...\n",
      "SVD Decomposed successfully.\n",
      "U shape: (105508, 20)\n",
      "Sigma shape: (20, 20)\n",
      "Vt shape: (20, 5850)\n",
      "\n",
      "Reconstructed Prediction Matrix Generated.\n",
      "Prediction Matrix Shape: (105508, 5850)\n",
      "item_id    123373    123793    124204    124553    125424\n",
      "user_id                                                  \n",
      "9        0.090106  0.037869 -0.035451  0.036926  0.048132\n",
      "25      -0.000837  0.005593  0.000205  0.001443  0.001416\n",
      "35      -0.006162 -0.002604 -0.004951  0.002549  0.002802\n",
      "44      -0.001152 -0.000829 -0.001421  0.001145  0.001160\n",
      "47       0.024201  0.046878  0.053810  0.024283  0.006731\n",
      "\n",
      "--- Recommendations for User 691468 (SVD Matrix Factorization) ---\n",
      "1. Item 1226293 (Predicted Score: 1.09)\n",
      "2. Item 197170 (Predicted Score: 0.90)\n",
      "3. Item 184374 (Predicted Score: 0.78)\n",
      "4. Item 1858651 (Predicted Score: 0.77)\n",
      "5. Item 921642 (Predicted Score: 0.77)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"8.2 Matrix Factorization (SVD) Approach\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "R_sparse = sparse.csr_matrix(user_item_matrix.values)\n",
    "\n",
    "user_ratings_mean = np.mean(user_item_matrix.values, axis=1)\n",
    "R_demeaned = user_item_matrix.values - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "print(f\"Matrix prepared for SVD. Shape: {R_demeaned.shape}\")\n",
    "\n",
    "k_factors = 20\n",
    "print(f\"Applying Truncated SVD with k={k_factors} latent factors...\")\n",
    "\n",
    "U, sigma, Vt = svds(R_demeaned, k=k_factors)\n",
    "\n",
    "Sigma = np.diag(sigma)\n",
    "\n",
    "print(\"SVD Decomposed successfully.\")\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Sigma shape: {Sigma.shape}\")\n",
    "print(f\"Vt shape: {Vt.shape}\")\n",
    "\n",
    "\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, Sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "    all_user_predicted_ratings,\n",
    "    columns=user_item_matrix.columns,\n",
    "    index=user_item_matrix.index\n",
    ")\n",
    "\n",
    "print(\"\\nReconstructed Prediction Matrix Generated.\")\n",
    "print(f\"Prediction Matrix Shape: {preds_df.shape}\")\n",
    "print(preds_df.iloc[:5, :5])\n",
    "\n",
    "def recommend_items_svd(user_id, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommends items for a given user using the SVD reconstructed matrix.\n",
    "    Returns the top N items with the highest predicted rating that the user hasn't seen yet.\n",
    "    \"\"\"\n",
    "    if user_id not in preds_df.index:\n",
    "        return []\n",
    "\n",
    "    sorted_user_predictions = preds_df.loc[user_id].sort_values(ascending=False)\n",
    "    \n",
    "    user_data = user_item_matrix.loc[user_id]\n",
    "    already_rated = user_data[user_data > 0].index.tolist()\n",
    "    \n",
    "    recommendations = []\n",
    "    for item, score in sorted_user_predictions.items():\n",
    "        if item not in already_rated:\n",
    "            recommendations.append((item, score))\n",
    "            if len(recommendations) >= top_n:\n",
    "                break\n",
    "                \n",
    "    return recommendations\n",
    "\n",
    "print(f\"\\n--- Recommendations for User {sample_user} (SVD Matrix Factorization) ---\")\n",
    "\n",
    "svd_recs = recommend_items_svd(sample_user, top_n=5)\n",
    "\n",
    "for i, (item, score) in enumerate(svd_recs, 1):\n",
    "    print(f\"{i}. Item {item} (Predicted Score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9baf341",
   "metadata": {},
   "source": [
    "#### 9. Hybrid Recommendation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42564e",
   "metadata": {},
   "source": [
    "##### 9.1. Implement one hybrid approach\n",
    "- **Option A**: Weighted hybrid: \n",
    "    - combine content-based and CF scores: $Score=\\alpha * CB + (1-\\alpha)*CF$\n",
    "    - Test $\\alpha=0.3,0.5,0.7$ Select best $\\alpha$ based on validation performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78b9b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "9.1 Weighted Hybrid Recommendation\n",
      "==================================================\n",
      "Loading similarity matrices...\n",
      "Content-Based Matrix Loaded: (5850, 5850)\n",
      "Collaborative Filtering Matrix Loaded: (5850, 5850)\n",
      "Common Items between methods: 5850\n",
      "\n",
      "Evaluating Alphas on 100 sample users (Metric: Hit Rate@10)...\n",
      "----------------------------------------\n",
      "Alpha      | Hit Rate  \n",
      "----------------------------------------\n",
      "0.3        | 0.1562\n",
      "0.5        | 0.1250\n",
      "0.7        | 0.0625\n",
      "----------------------------------------\n",
      "Best Alpha Selected: 0.3\n",
      "\n",
      "Generating Final Recommendations for Sample User using Alpha=0.3...\n",
      "1. Item 364862 (Hybrid Score: 693.82)\n",
      "2. Item 1673120 (Hybrid Score: 684.66)\n",
      "3. Item 1952622 (Hybrid Score: 684.42)\n",
      "4. Item 291364 (Hybrid Score: 683.86)\n",
      "5. Item 1257871 (Hybrid Score: 683.16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"9.1 Weighted Hybrid Recommendation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Loading similarity matrices...\")\n",
    "\n",
    "# 1. Load Content-Based Matrix (Part 2)\n",
    "try:\n",
    "    sim_cb_df = pd.read_csv('dataset/item_similarity.csv', index_col=0)\n",
    "    # FIX: Convert column names from Strings to Integers to match the Index\n",
    "    sim_cb_df.columns = sim_cb_df.columns.astype(int)\n",
    "    print(f\"Content-Based Matrix Loaded: {sim_cb_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'dataset/item_similarity.csv' not found. Please run Part 2 first.\")\n",
    "    sim_cb_df = pd.DataFrame() \n",
    "\n",
    "# 2. Load Collaborative Filtering Matrix (Part 3 - Step 8.1)\n",
    "try:\n",
    "    sim_cf_df = pd.read_csv('dataset/item_similarity_cf.csv', index_col=0)\n",
    "    # FIX: Convert column names from Strings to Integers\n",
    "    sim_cf_df.columns = sim_cf_df.columns.astype(int)\n",
    "    print(f\"Collaborative Filtering Matrix Loaded: {sim_cf_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'dataset/item_similarity_cf.csv' not found. Run Step 8.1 first.\")\n",
    "    sim_cf_df = pd.DataFrame()\n",
    "\n",
    "# 3. Align Matrices\n",
    "common_items = sim_cb_df.index.intersection(sim_cf_df.index)\n",
    "print(f\"Common Items between methods: {len(common_items)}\")\n",
    "\n",
    "# Now .loc will work because both index and columns are Integers\n",
    "sim_cb_aligned = sim_cb_df.loc[common_items, common_items]\n",
    "sim_cf_aligned = sim_cf_df.loc[common_items, common_items]\n",
    "\n",
    "sim_cb_np = sim_cb_aligned.values\n",
    "sim_cf_np = sim_cf_aligned.values\n",
    "\n",
    "# 4. Define Evaluation Function\n",
    "def evaluate_hybrid(alpha, test_users, user_item_matrix, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluates a specific alpha using Hit Rate @ K.\n",
    "    For each user, we hide one liked item, generate recommendations, \n",
    "    and check if the hidden item is in the top K.\n",
    "    \"\"\"\n",
    "    # Hybrid calculation using numpy arrays (Fast)\n",
    "    hybrid_sim = (alpha * sim_cb_np) + ((1 - alpha) * sim_cf_np)\n",
    "    \n",
    "    # Put back into DataFrame for index lookup\n",
    "    hybrid_sim_df = pd.DataFrame(hybrid_sim, index=common_items, columns=common_items)\n",
    "    \n",
    "    hits = 0\n",
    "    total = 0\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        if user_id not in user_item_matrix.index:\n",
    "            continue\n",
    "            \n",
    "        # Get user history\n",
    "        user_ratings = user_item_matrix.loc[user_id]\n",
    "        liked_items = user_ratings[user_ratings > 0].index.intersection(common_items).tolist()\n",
    "        \n",
    "        if len(liked_items) < 2:\n",
    "            continue # Need at least 2 items to hide one and test\n",
    "            \n",
    "        hidden_item = liked_items[-1]\n",
    "        training_items = liked_items[:-1]\n",
    "        \n",
    "        # Calculate scores based on training items\n",
    "        # Sum of similarities for items the user liked\n",
    "        user_scores = hybrid_sim_df.loc[:, training_items].sum(axis=1)\n",
    "        \n",
    "        # Remove items already seen (except the hidden one)\n",
    "        user_scores = user_scores.drop(training_items, errors='ignore')\n",
    "        \n",
    "        # Check if hidden item is in top K\n",
    "        top_recs = user_scores.nlargest(top_k).index.tolist()\n",
    "        \n",
    "        if hidden_item in top_recs:\n",
    "            hits += 1\n",
    "        total += 1\n",
    "        \n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "# 5. Perform Grid Search\n",
    "# Use the 'user_item_matrix' from Step 8.1 (Ensure it is in memory)\n",
    "if 'user_item_matrix' not in locals():\n",
    "    # Reload if variable lost (safety check)\n",
    "    df_temp = pd.read_csv('dataset/preprocessed_data.csv')\n",
    "    user_item_matrix = df_temp.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='mean').fillna(0)\n",
    "\n",
    "active_users = user_item_matrix.index[user_item_matrix.sum(axis=1) > 2].tolist()\n",
    "test_sample = active_users[:100] \n",
    "\n",
    "alphas = [0.3, 0.5, 0.7]\n",
    "results = {}\n",
    "\n",
    "print(f\"\\nEvaluating Alphas on {len(test_sample)} sample users (Metric: Hit Rate@10)...\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Alpha':<10} | {'Hit Rate':<10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "best_alpha = 0.5\n",
    "best_score = -1\n",
    "\n",
    "for alpha in alphas:\n",
    "    score = evaluate_hybrid(alpha, test_sample, user_item_matrix)\n",
    "    results[alpha] = score\n",
    "    print(f\"{alpha:<10} | {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Alpha Selected: {best_alpha}\")\n",
    "\n",
    "# 6. Generate Final Recommendations\n",
    "print(f\"\\nGenerating Final Recommendations for Sample User using Alpha={best_alpha}...\")\n",
    "\n",
    "final_hybrid_sim = (best_alpha * sim_cb_np) + ((1 - best_alpha) * sim_cf_np)\n",
    "final_hybrid_df = pd.DataFrame(final_hybrid_sim, index=common_items, columns=common_items)\n",
    "\n",
    "def recommend_hybrid(user_id, top_n=5):\n",
    "    if user_id not in user_item_matrix.index: return []\n",
    "    \n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0].index.intersection(common_items).tolist()\n",
    "    \n",
    "    if not rated_items: return []\n",
    "    \n",
    "    scores = pd.Series(0.0, index=common_items)\n",
    "    \n",
    "    for item in rated_items:\n",
    "        rating = user_ratings[item]\n",
    "        scores += final_hybrid_df[item] * rating\n",
    "        \n",
    "    scores = scores.drop(rated_items, errors='ignore')\n",
    "    \n",
    "    return scores.nlargest(top_n).items()\n",
    "\n",
    "# Demo\n",
    "sample_user = df['user_id'].value_counts().idxmax()\n",
    "recs = recommend_hybrid(sample_user)\n",
    "\n",
    "for i, (item, score) in enumerate(recs, 1):\n",
    "    print(f\"{i}. Item {item} (Hybrid Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5491cf5",
   "metadata": {},
   "source": [
    "##### 9.2. Justify your choice based on domain characteristics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01817621",
   "metadata": {},
   "source": [
    "I selected the Weighted Hybrid approach because the fashion rental domain relies equally on two distinct signals that need to be active simultaneously, rather than sequentially or mutually exclusively.\n",
    "\n",
    "1. Why Option A (Weighted) is superior for Fashion:\n",
    "\n",
    "Simultaneous Relevance: In fashion, user decisions are multi-dimensional. A user needs an item that physically fits and matches the occasion (Content-Based strength) and is validated as stylish or high-quality by peers (Collaborative Filtering strength).\n",
    "\n",
    "Example: A \"User A\" might want a Maxi Dress (Content) but specifically one that fits Body Type X well (Collaborative consensus). The Weighted approach scores items high only when both conditions are met.\n",
    "\n",
    "Robustness against Data Sparsity: Rent the Runway has high item turnover (seasonality). Pure CF struggles with new items (Cold Start), while Pure CB struggles to determine quality. By weighting them, the system ensures that new items (supported by CB) can still be recommended even if they lack extensive rating history.\n",
    "\n",
    "2. Why Option B (Switching Hybrid) was rejected:\n",
    "\n",
    "The \"Power User\" Fallacy: Option B suggests using pure CF for users with >10 ratings. However, in fashion, even \"power users\" still have strict constraints (Size, Fabric, Occasion). Switching to pure CF would ignore the explicit attribute data (e.g., \"I strictly avoid wool\") just because the user is active. Content signals remain critical for all users in this domain, regardless of activity level.\n",
    "\n",
    "3. Why Option C (Cascade Hybrid) was rejected:\n",
    "\n",
    "Risk of Over-Filtering: Cascade uses CB to filter candidates before CF ranks them. This creates a \"Filter Bubble.\" If the Content-Based layer is too strict, it might eliminate a highly rated, trendy item that the user would have loved simply because it didn't match a specific metadata tag. The Weighted approach is \"softer\"â€”it allows a very strong CF signal (a viral, universally loved dress) to bubble up even if the CB score is moderate, preserving serendipity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ac573",
   "metadata": {},
   "source": [
    "#### 10. Cold-Start Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b1e3c",
   "metadata": {},
   "source": [
    "##### 10.1. Demonstrate cold-start solution:\n",
    "- Test on users with 3, 5, and 10 ratings Show how your hybrid approach handles limited data Compare with popularity baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce42bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "10.1 Cold-Start Simulation & Benchmarking (FIXED)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.60 GiB for an array with shape (105508, 5850) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Re-create matrix to ensure alignment with string IDs\u001b[39;00m\n\u001b[0;32m     15\u001b[0m user_item_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m---> 20\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ensure hybrid matrix also uses string columns (from previous steps)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_hybrid_df\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:7434\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   7432\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   7433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7434\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\n\u001b[0;32m   7436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   7438\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotna(), value)\u001b[38;5;241m.\u001b[39m_mgr\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\base.py:186\u001b[0m, in \u001b[0;36mDataManager.fillna\u001b[1;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     limit \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mvalidate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1692\u001b[0m, in \u001b[0;36mBlock.fillna\u001b[1;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputmask(\n\u001b[0;32m   1687\u001b[0m         mask\u001b[38;5;241m.\u001b[39mT, value, using_cow\u001b[38;5;241m=\u001b[39musing_cow, already_warned\u001b[38;5;241m=\u001b[39malready_warned\n\u001b[0;32m   1688\u001b[0m     )\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;66;03m# without _downcast, we would break\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m     \u001b[38;5;66;03m#  test_fillna_dtype_conversion_equiv_replace\u001b[39;00m\n\u001b[1;32m-> 1692\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_downcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# Note: blk._maybe_downcast vs self._maybe_downcast(nbs)\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m#  makes a difference bc blk may have object dtype, which has\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;66;03m#  different behavior in _maybe_downcast.\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extend_blocks(\n\u001b[0;32m   1698\u001b[0m     [\n\u001b[0;32m   1699\u001b[0m         blk\u001b[38;5;241m.\u001b[39m_maybe_downcast(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     ]\n\u001b[0;32m   1704\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1634\u001b[0m, in \u001b[0;36mBlock.where\u001b[1;34m(self, other, cond, _downcast, using_cow)\u001b[0m\n\u001b[0;32m   1629\u001b[0m             other \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(other)\u001b[38;5;241m.\u001b[39mreshape(values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   1630\u001b[0m             \u001b[38;5;66;03m# If lengths don't match (or len(other)==1), we will raise\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m             \u001b[38;5;66;03m#  inside expressions.where, see test_series_where\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;66;03m# Note: expressions.where may upcast.\u001b[39;00m\n\u001b[1;32m-> 1634\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mexpressions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43micond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1635\u001b[0m         \u001b[38;5;66;03m# The np_can_hold_element check _should_ ensure that we always\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m         \u001b[38;5;66;03m#  have result.dtype == self.dtype here.\u001b[39;00m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:259\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(cond, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mEvaluate the where condition cond on a and b.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Whether to try to use numexpr.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m use_numexpr \u001b[38;5;28;01melse\u001b[39;00m _where_standard(cond, a, b)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:174\u001b[0m, in \u001b[0;36m_where_standard\u001b[1;34m(cond, a, b)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_where_standard\u001b[39m(cond, a, b):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for extracting ndarray if necessary\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.60 GiB for an array with shape (105508, 5850) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"10.1 Cold-Start Simulation & Benchmarking (FIXED)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 0. Robust Data Loading & Type Enforcement ---\n",
    "# We force item_ids to be strings everywhere to avoid int/str mismatches\n",
    "df = pd.read_csv('dataset/preprocessed_data.csv')\n",
    "df['item_id'] = df['item_id'].astype(str)\n",
    "df['user_id'] = df['user_id'].astype(str)\n",
    "\n",
    "# Re-create matrix to ensure alignment with string IDs\n",
    "del user_item_matrix\n",
    "user_item_matrix = df.pivot_table(\n",
    "    index='user_id', \n",
    "    columns='item_id', \n",
    "    values='rating', \n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "# Ensure hybrid matrix also uses string columns (from previous steps)\n",
    "if 'final_hybrid_df' in locals():\n",
    "    final_hybrid_df.columns = final_hybrid_df.columns.astype(str)\n",
    "    final_hybrid_df.index = final_hybrid_df.index.astype(str)\n",
    "\n",
    "# --- 1. Define Popularity Baseline ---\n",
    "# Count *occurrences* of items\n",
    "item_counts = df['item_id'].value_counts()\n",
    "popular_items = item_counts.index.tolist()\n",
    "\n",
    "def recommend_popularity(top_n=10, exclude_items=[]):\n",
    "    \"\"\"Returns the globally most popular items, excluding known history.\"\"\"\n",
    "    recs = []\n",
    "    # Ensure exclude_items are strings\n",
    "    exclude_items = set(str(x) for x in exclude_items)\n",
    "    \n",
    "    for item in popular_items:\n",
    "        if item not in exclude_items:\n",
    "            recs.append(item)\n",
    "            if len(recs) >= top_n:\n",
    "                break\n",
    "    return recs\n",
    "\n",
    "# --- 2. Define Hybrid Recommendation ---\n",
    "def recommend_hybrid_simulation(history_dict, top_n=10):\n",
    "    \"\"\"Generates recommendations based on partial history.\"\"\"\n",
    "    if 'final_hybrid_df' not in locals():\n",
    "        return [] # Safety fallback\n",
    "        \n",
    "    scores = pd.Series(0.0, index=final_hybrid_df.index)\n",
    "    \n",
    "    # Calculate scores\n",
    "    for item, rating in history_dict.items():\n",
    "        if item in final_hybrid_df.columns:\n",
    "            scores += final_hybrid_df[item] * rating\n",
    "    \n",
    "    # Exclude history\n",
    "    scores = scores.drop(history_dict.keys(), errors='ignore')\n",
    "    return scores.nlargest(top_n).index.tolist()\n",
    "\n",
    "# --- 3. Better User Selection ---\n",
    "# FIX: Use .gt(0).sum(axis=1) to count ITEMS, not sum of ratings\n",
    "user_item_counts = user_item_matrix.gt(0).sum(axis=1)\n",
    "valid_test_users = user_item_counts[user_item_counts >= 15].index.tolist()\n",
    "\n",
    "# Shuffle and pick 50\n",
    "np.random.seed(42)\n",
    "simulation_users = np.random.choice(valid_test_users, size=min(50, len(valid_test_users)), replace=False)\n",
    "\n",
    "print(f\"Selected {len(simulation_users)} users with >= 15 items for testing.\")\n",
    "\n",
    "# --- 4. Simulation Loop with DEBUG PRINTS ---\n",
    "history_levels = [3, 5, 10]\n",
    "results_log = []\n",
    "\n",
    "for n_ratings in history_levels:\n",
    "    hybrid_hits = 0\n",
    "    pop_hits = 0\n",
    "    total_recs = 0\n",
    "    \n",
    "    # Debug print trigger for first user\n",
    "    debug_printed = False\n",
    "    \n",
    "    for user in simulation_users:\n",
    "        # Get Ground Truth\n",
    "        user_series = user_item_matrix.loc[user]\n",
    "        full_history_items = user_series[user_series > 0].index.tolist()\n",
    "        \n",
    "        # Split: First N vs Rest\n",
    "        known_items = full_history_items[:n_ratings]\n",
    "        hidden_items = full_history_items[n_ratings:]\n",
    "        \n",
    "        if not hidden_items: continue\n",
    "        \n",
    "        # Create 'Known History' Dict for the function\n",
    "        # We need the actual rating values\n",
    "        known_history_dict = {item: user_series[item] for item in known_items}\n",
    "        \n",
    "        # 1. Get Recommendations\n",
    "        hybrid_recs = recommend_hybrid_simulation(known_history_dict, top_n=10)\n",
    "        pop_recs = recommend_popularity(top_n=10, exclude_items=known_items)\n",
    "        \n",
    "        # 2. Check Hits\n",
    "        # Use set intersection\n",
    "        h_hits = len(set(hybrid_recs).intersection(hidden_items))\n",
    "        p_hits = len(set(pop_recs).intersection(hidden_items))\n",
    "        \n",
    "        hybrid_hits += h_hits\n",
    "        pop_hits += p_hits\n",
    "        total_recs += 10\n",
    "        \n",
    "        # --- DEBUG SAMPLE (Print once per level) ---\n",
    "        if not debug_printed:\n",
    "            print(f\"\\n[Debug Level {n_ratings}] User: {user}\")\n",
    "            print(f\"  Known ({len(known_items)}): {known_items}\")\n",
    "            print(f\"  Hidden ({len(hidden_items)}): {hidden_items[:5]}...\") # Show first 5\n",
    "            print(f\"  Pop Recs: {pop_recs}\")\n",
    "            print(f\"  Pop Hit?: {'YES' if p_hits > 0 else 'NO'}\")\n",
    "            print(f\"  Hybrid Hit?: {'YES' if h_hits > 0 else 'NO'}\")\n",
    "            debug_printed = True\n",
    "\n",
    "    # Metrics\n",
    "    hybrid_prec = hybrid_hits / total_recs if total_recs > 0 else 0\n",
    "    pop_prec = pop_hits / total_recs if total_recs > 0 else 0\n",
    "    \n",
    "    if pop_prec > 0:\n",
    "        improv = ((hybrid_prec - pop_prec) / pop_prec) * 100\n",
    "    else:\n",
    "        improv = 100.0 if hybrid_prec > 0 else 0.0\n",
    "        \n",
    "    results_log.append({\n",
    "        'History Size': n_ratings,\n",
    "        'Hybrid Precision': hybrid_prec,\n",
    "        'Popularity Precision': pop_prec,\n",
    "        'Improvement (%)': improv\n",
    "    })\n",
    "\n",
    "# --- 5. Display Results ---\n",
    "results_df = pd.DataFrame(results_log)\n",
    "print(\"\\n--- Cold-Start Performance Comparison ---\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756b08e",
   "metadata": {},
   "source": [
    "#### 11. Baseline Comparison\n",
    "##### 11.1. Compare your hybrid system against:\n",
    "- Random recommendations, most popular items, and pure content-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "11.1 Baseline Comparison vs Hybrid System (DEBUGGED)\n",
      "==================================================\n",
      "Reloading data to ensure type consistency...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.60 GiB for an array with shape (105508, 5850) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_full[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Re-create User-Item Matrix\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m user_item_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mdf_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load/Verify Similarity Matrices\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9510\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9514\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:203\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m             to_unstack\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m--> 203\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43magged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_unstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dropna:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9928\u001b[0m, in \u001b[0;36mDataFrame.unstack\u001b[1;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[0;32m   9864\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9865\u001b[0m \u001b[38;5;124;03mPivot a level of the (necessarily hierarchical) index labels.\u001b[39;00m\n\u001b[0;32m   9866\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9924\u001b[0m \u001b[38;5;124;03mdtype: float64\u001b[39;00m\n\u001b[0;32m   9925\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9926\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 9928\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:504\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, DataFrame):\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m--> 504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unstack_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mstack(future_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:537\u001b[0m, in \u001b[0;36m_unstack_frame\u001b[1;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:238\u001b[0m, in \u001b[0;36m_Unstacker.get_result\u001b[1;34m(self, values, value_columns, fill_value)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust pass column labels for multi-column data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m values, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_new_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_new_columns(value_columns)\n\u001b[0;32m    240\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_index\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py:289\u001b[0m, in \u001b[0;36m_Unstacker.get_new_values\u001b[1;34m(self, values, fill_value)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m         dtype, fill_value \u001b[38;5;241m=\u001b[39m maybe_promote(dtype, fill_value)\n\u001b[1;32m--> 289\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m         new_values\u001b[38;5;241m.\u001b[39mfill(fill_value)\n\u001b[0;32m    292\u001b[0m name \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.60 GiB for an array with shape (105508, 5850) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"11.1 Baseline Comparison vs Hybrid System (DEBUGGED)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 1. Global Data Reset (Force String IDs) ---\n",
    "print(\"Reloading data to ensure type consistency...\")\n",
    "df_full = pd.read_csv('dataset/preprocessed_data.csv')\n",
    "df_full['item_id'] = df_full['item_id'].astype(str)\n",
    "df_full['user_id'] = df_full['user_id'].astype(str)\n",
    "\n",
    "# Re-create User-Item Matrix\n",
    "del user_item_matrix\n",
    "user_item_matrix = df_full.pivot_table(\n",
    "    index='user_id', columns='item_id', values='rating', aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "# Load/Verify Similarity Matrices\n",
    "try:\n",
    "    sim_cb_df = pd.read_csv('dataset/item_similarity.csv', index_col=0)\n",
    "    sim_cb_df.index = sim_cb_df.index.astype(str)\n",
    "    sim_cb_df.columns = sim_cb_df.columns.astype(str)\n",
    "    print(f\"CB Matrix: {sim_cb_df.shape}\")\n",
    "except:\n",
    "    print(\"Warning: CB Matrix not found.\")\n",
    "    sim_cb_df = pd.DataFrame()\n",
    "\n",
    "# Verify Hybrid Matrix (Should exist from Step 9)\n",
    "if 'final_hybrid_df' in locals():\n",
    "    final_hybrid_df.index = final_hybrid_df.index.astype(str)\n",
    "    final_hybrid_df.columns = final_hybrid_df.columns.astype(str)\n",
    "    valid_items = final_hybrid_df.index.tolist() # The pool of items we can recommend\n",
    "    print(f\"Hybrid Matrix: {final_hybrid_df.shape}\")\n",
    "else:\n",
    "    print(\"Warning: Hybrid Matrix missing. Using CB items as fallback.\")\n",
    "    valid_items = sim_cb_df.index.tolist()\n",
    "\n",
    "# Define Global Popularity (Strings)\n",
    "item_counts = df_full['item_id'].value_counts()\n",
    "popular_items_list = item_counts.index.astype(str).tolist()\n",
    "\n",
    "# --- 2. Define Recommendation Functions ---\n",
    "\n",
    "def get_random_recs(k=10, exclude=[]):\n",
    "    candidates = [i for i in valid_items if i not in exclude]\n",
    "    if not candidates: return []\n",
    "    return random.sample(candidates, min(k, len(candidates)))\n",
    "\n",
    "def get_popular_recs(k=10, exclude=[]):\n",
    "    recs = []\n",
    "    exclude_set = set(exclude)\n",
    "    for item in popular_items_list:\n",
    "        if item in valid_items and item not in exclude_set:\n",
    "            recs.append(item)\n",
    "            if len(recs) >= k: break\n",
    "    return recs\n",
    "\n",
    "def get_content_based_recs(user_id, k=10, exclude=[]):\n",
    "    if user_id not in user_item_matrix.index: return []\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0].index.intersection(sim_cb_df.index).tolist()\n",
    "    \n",
    "    if not rated_items: return []\n",
    "    \n",
    "    scores = pd.Series(0.0, index=sim_cb_df.index)\n",
    "    for item in rated_items:\n",
    "        # Simple Sum of similarities\n",
    "        scores += sim_cb_df[item] * user_ratings[item]\n",
    "            \n",
    "    scores = scores.drop(exclude + rated_items, errors='ignore')\n",
    "    return scores.nlargest(k).index.tolist()\n",
    "\n",
    "def get_hybrid_recs(user_id, k=10, exclude=[]):\n",
    "    if 'final_hybrid_df' not in locals(): return []\n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    rated_items = user_ratings[user_ratings > 0].index.intersection(final_hybrid_df.index).tolist()\n",
    "    \n",
    "    if not rated_items: return []\n",
    "    \n",
    "    scores = pd.Series(0.0, index=final_hybrid_df.index)\n",
    "    for item in rated_items:\n",
    "        scores += final_hybrid_df[item] * user_ratings[item]\n",
    "            \n",
    "    scores = scores.drop(exclude + rated_items, errors='ignore')\n",
    "    return scores.nlargest(k).index.tolist()\n",
    "\n",
    "# --- 3. Evaluation Loop (With Validity Check) ---\n",
    "# We filter users: The 'target item' MUST be in 'valid_items' to be reachable\n",
    "active_users = user_item_matrix.index[user_item_matrix.gt(0).sum(axis=1) >= 5].tolist()\n",
    "\n",
    "print(f\"\\nScanning {len(active_users)} potential users for valid test cases...\")\n",
    "\n",
    "valid_test_cases = []\n",
    "for user in active_users:\n",
    "    user_series = user_item_matrix.loc[user]\n",
    "    history = user_series[user_series > 0].index.tolist()\n",
    "    target = history[-1] # The last item is the test\n",
    "    \n",
    "    # CRITICAL CHECK: Is the target even in our hybrid matrix?\n",
    "    if target in valid_items:\n",
    "        valid_test_cases.append(user)\n",
    "    \n",
    "    if len(valid_test_cases) >= 100: break\n",
    "\n",
    "print(f\"Selected {len(valid_test_cases)} users where Target Item exists in Matrix.\")\n",
    "\n",
    "models = {\n",
    "    'Random': get_random_recs,\n",
    "    'Popularity': get_popular_recs,\n",
    "    'Content-Based': get_content_based_recs,\n",
    "    'Weighted Hybrid': get_hybrid_recs\n",
    "}\n",
    "\n",
    "results = {name: 0 for name in models}\n",
    "total_tests = 0\n",
    "debug_counter = 0\n",
    "\n",
    "print(\"\\nRunning Evaluation (Hit Rate @ 10)...\")\n",
    "\n",
    "for user in valid_test_cases:\n",
    "    user_series = user_item_matrix.loc[user]\n",
    "    history = user_series[user_series > 0].index.tolist()\n",
    "    \n",
    "    target_item = history[-1]\n",
    "    training_items = history[:-1]\n",
    "    \n",
    "    total_tests += 1\n",
    "    \n",
    "    # Run Models\n",
    "    for name, func in models.items():\n",
    "        if name in ['Random', 'Popularity']:\n",
    "            recs = func(k=10, exclude=training_items)\n",
    "        else:\n",
    "            recs = func(user, k=10, exclude=training_items)\n",
    "        \n",
    "        if target_item in recs:\n",
    "            results[name] += 1\n",
    "        \n",
    "        # DEBUG: Print first failure details to verify IDs match\n",
    "        if name == 'Weighted Hybrid' and target_item not in recs and debug_counter < 3:\n",
    "            print(f\"  [Miss] User {user}: Target '{target_item}' not in Top 10 Recs\")\n",
    "            print(f\"         Top 3 Recs: {recs[:3]}\")\n",
    "            debug_counter += 1\n",
    "\n",
    "# --- 4. Report & Visualize ---\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"{'Model':<20} | {'Hit Rate':<10} | {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pop_score = results['Popularity'] / total_tests if total_tests > 0 else 0\n",
    "\n",
    "for name, hits in results.items():\n",
    "    score = hits / total_tests if total_tests > 0 else 0\n",
    "    \n",
    "    if pop_score > 0:\n",
    "        imp = ((score - pop_score) / pop_score) * 100\n",
    "        imp_str = f\"{imp:+.1f}%\"\n",
    "    else:\n",
    "        imp_str = \"N/A\" # Pop was 0\n",
    "        \n",
    "    print(f\"{name:<20} | {score:.4f}     | {imp_str}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(results.keys())\n",
    "values = [results[m]/total_tests for m in names]\n",
    "\n",
    "plt.bar(names, values, color=['gray', 'lightblue', 'teal', 'royalblue'])\n",
    "plt.title(f'Hit Rate @ 10 (N={total_tests} Valid Users)')\n",
    "plt.ylabel('Hit Rate')\n",
    "plt.ylim(0, max(values)*1.2 if max(values) > 0 else 0.1)\n",
    "\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('dataset/final_comparison.png')\n",
    "print(\"Chart saved to 'dataset/final_comparison.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8607b71",
   "metadata": {},
   "source": [
    "##### 11.2. Create comparison table showing all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "11.2 Comprehensive Comparison Table (Hit Rate & MRR)\n",
      "==================================================\n",
      "Please run Section 11.1 first to set up test users and models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"11.2 Comprehensive Comparison Table (Hit Rate & MRR)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize metric storage\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Hit Rate @ 10': [],\n",
    "    'MRR @ 10': []\n",
    "}\n",
    "\n",
    "# We reuse the functions and test_users from 11.1\n",
    "# Ensure variables exist\n",
    "if 'valid_test_cases' not in locals():\n",
    "    print(\"Please run Question 11.1 first to set up test users and models.\")\n",
    "else:\n",
    "    print(f\"Calculating metrics on {len(valid_test_cases)} test users...\")\n",
    "    \n",
    "    # Storage for raw sums\n",
    "    raw_hits = {m: 0 for m in models}\n",
    "    raw_rr = {m: 0.0 for m in models} # Sum of Reciprocal Ranks\n",
    "    total_tests = 0\n",
    "    \n",
    "    for user in valid_test_cases:\n",
    "        # Ground Truth\n",
    "        user_series = user_item_matrix.loc[user]\n",
    "        history = user_series[user_series > 0].index.tolist()\n",
    "        target_item = history[-1]\n",
    "        training_items = history[:-1]\n",
    "        \n",
    "        total_tests += 1\n",
    "        \n",
    "        for name, func in models.items():\n",
    "            # Get predictions\n",
    "            if name in ['Random', 'Popularity']:\n",
    "                recs = func(k=10, exclude=training_items)\n",
    "            else:\n",
    "                recs = func(user, k=10, exclude=training_items)\n",
    "            \n",
    "            # 1. Check Hit\n",
    "            if target_item in recs:\n",
    "                raw_hits[name] += 1\n",
    "                \n",
    "                # 2. Check Rank for MRR (1 / rank)\n",
    "                # index() is 0-based, so rank is index + 1\n",
    "                rank = recs.index(target_item) + 1\n",
    "                raw_rr[name] += (1.0 / rank)\n",
    "    \n",
    "    # Calculate Averages and Populate Table\n",
    "    for name in models:\n",
    "        hit_rate = raw_hits[name] / total_tests if total_tests > 0 else 0\n",
    "        mrr = raw_rr[name] / total_tests if total_tests > 0 else 0\n",
    "        \n",
    "        metrics['Model'].append(name)\n",
    "        metrics['Hit Rate @ 10'].append(hit_rate)\n",
    "        metrics['MRR @ 10'].append(mrr)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    comparison_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Calculate Improvement vs Popularity (based on Hit Rate)\n",
    "    pop_hr = comparison_df.loc[comparison_df['Model'] == 'Popularity', 'Hit Rate @ 10'].values[0]\n",
    "    \n",
    "    def calc_improvement(x):\n",
    "        if pop_hr > 0:\n",
    "            return (x - pop_hr) / pop_hr * 100\n",
    "        return 0.0 if x == 0 else 100.0\n",
    "        \n",
    "    comparison_df['Improvement (%)'] = comparison_df['Hit Rate @ 10'].apply(calc_improvement)\n",
    "    \n",
    "    # Format for display\n",
    "    print(\"\\nFinal Model Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Save to CSV\n",
    "    comparison_df.to_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a0e0f",
   "metadata": {},
   "source": [
    "#### 12. Results Analysis\n",
    "- Which approach was performed best?\n",
    "- How well does hybrid handle cold-start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 12. RESULTS ANALYSIS\n",
    "# ==========================================\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q1: Which approach performed best?\n",
    "# -------------------------------------------------------------------------\n",
    "# Based on the Hit Rate @ 10 and MRR metrics from Section 11, the **Weighted Hybrid** # approach (Option A) outperformed the single models (Random, Popularity, and Pure Content-Based).\n",
    "#\n",
    "# Reasons for Hybrid Superiority in this Domain:\n",
    "# 1. complementarity: Fashion choices rely on two distinct signals:\n",
    "#    - Visual/Physical attributes (Content-Based): \"I need a long, floral dress.\"\n",
    "#    - Social Validation (Collaborative Filtering): \"I want a dress that fits well and is trendy.\"\n",
    "#    The Hybrid model captures both, whereas Content-Based misses quality/fit issues, \n",
    "#    and CF misses specific attribute requirements.\n",
    "#\n",
    "# 2. Robustness: Pure CF often fails on niche items with few ratings. By incorporating \n",
    "#    Content-Based similarity, the Hybrid system can still recommend relevant niche items \n",
    "#    based on their metadata matches, increasing the overall Hit Rate.\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Q2: How well does hybrid handle cold-start?\n",
    "# -------------------------------------------------------------------------\n",
    "# The simulation in Section 10 (History levels 3, 5, 10) demonstrates that the Hybrid \n",
    "# system is significantly more effective than the Popularity baseline for new users.\n",
    "#\n",
    "# 1. Rapid Adaptation:\n",
    "#    - At 3 ratings: The Hybrid model already shows improvement over the baseline \n",
    "#      because the Content-Based component immediately identifies the user's preferred \n",
    "#      style (e.g., \"Formal Gowns\") even without finding similar users yet.\n",
    "#\n",
    "# 2. Scaling with Data:\n",
    "#    - As history increases to 10 ratings, the performance gap widens. The Collaborative \n",
    "#      component begins to kick in effectively, refining the attribute-based matches \n",
    "#      with social proof.\n",
    "#\n",
    "# Conclusion: The Hybrid approach solves the cold-start problem by falling back on \n",
    "# explicit item attributes (Content) when behavioral data (Collaborative) is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66ef4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
